{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Feedback For Product-Focused Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> John Newcomb, Doug Mill, Andrew Marinelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large compaines, broad user feedback generally takes on a relatively stiff form: The company issues a feedback survey with some sort of coupon reward for completion, and the user has the option to complete or not complete. There are several problems with this approach. First of all, voluntary feedback often times does not accurate reflect true sentiment. People generally feel the need to be nice, so they do not give their true thoughts. Negative experiences become neutral, neutral experiences become positive, and positive experiences become legendary. \n",
    "\n",
    "However, new means of information aggregation enable us to on user feedback in a natural way, where users do not feel surveilled. Using information streams from twitter API, along with an in-house contructed sentiment analysis tool, we aim to provide real-time tweet flagging for copmanies interested in getting synchronous and asynchronous honest feedback on their products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering user feedback can be both difficult and deceiving. Additionally, tracking down and tackling bad PR has always been anything but predictable. However, in the digital age, we are able to monitor social media sites for indications of bad press. The idea is simple: Hook up to the twitter API, search for tweets related to your company via keyword (so, for exmaple, Apple might be interested in any tweet containing \"iPad\", \"iPhone\", etc), then have your user experience team sort through them as to identify strengths and weaknesses in the copmany. However, with the application of NLP, it is actually possible not only to bin the tweets but also to analyse them for sentiment. The implication of this is that a user experience team could get a constant stream of information regarding the shortcoming of their product, enabling more rapid iterations and quicker responses. The overall result: decreased constomer churn, increased user experience productivity, which, as you might guess, leads to overall increase in bottom line revenues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do it? We begin with a dataset of tweets from South By Southwest 2013, a popular music festival in Austin, TX, where high tech companies make a habit of appearing to flaunt their latest and trendiest updates. The tweets are compiled by hashtags (#sxsw as well as others, case insensitive). First, we generate metadata per tweet, and clean the dataset up overall. Next, we go through an iterative modeling process, finally agreeing on a model per pre-established metric evaulation. Finally, we cross validate our results, and eventually test our product on unseen data to confirm our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with the basic necessary EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-c09b5d635a8c>:6: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/westonnewcomb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we're working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.               \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.                                                               \n",
       "3  @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw                                                            \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)           \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0  iPhone                           \n",
       "1  iPad or iPhone App               \n",
       "2  iPad                             \n",
       "3  iPad or iPhone App               \n",
       "4  Google                           \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0  Negative emotion                                   \n",
       "1  Positive emotion                                   \n",
       "2  Positive emotion                                   \n",
       "3  Negative emotion                                   \n",
       "4  Positive emotion                                   "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data in as pandas DataFrame object\n",
    "df = pd.read_csv('data/data.csv', encoding='unicode_escape')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data appears to consist of three columns, each relatively simple to understand. Our first data is a tweet, followed by the product at which the tweet is directed, and finally the sentiment of that tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, let's go ahead and rename the columns for the sake of simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for simplicity\n",
    "columns_dict = {'tweet_text':'tweet',\n",
    "                'emotion_in_tweet_is_directed_at':'product',\n",
    "                'is_there_an_emotion_directed_at_a_brand_or_product':'emotion_response'}\n",
    "\n",
    "df = df.rename(columns=columns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   tweet             9092 non-null   object\n",
      " 1   product           3291 non-null   object\n",
      " 2   emotion_response  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet               1   \n",
       "product             5802\n",
       "emotion_response    0   \n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we deal with this nightmare of a situation with the product column, let's take a look at the null-valued tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>emotion_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet product                    emotion_response\n",
       "6  NaN   NaN     No emotion toward brand or product"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['tweet'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears likely to be a data input error. We can drop that row from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we take a look at the product column next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                        81 \n",
       "Android                            78 \n",
       "Other Apple product or service     35 \n",
       "Name: product, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6380334359876815"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['product'].isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though we are do not have much information about the products - we're missing values in nearly two-thirds of the rows in the 'product' column. Perhaps this is a mistake in the dataset. In order to circumvent this issue, let's fill in the values if the tweet contains words indicative of the company involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_words = ['iphone', 'ipad', 'apple']\n",
    "google_words = ['google', 'android']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brand_classifier(tweet):\n",
    "    \n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    google = any(g in tweet for g in google_words)\n",
    "    apple = any(a in tweet for a in apple_words)\n",
    "    \n",
    "    if (apple & google):\n",
    "        return 'both'\n",
    "    elif apple:\n",
    "        return 'apple'\n",
    "    elif google:\n",
    "        return 'google'\n",
    "    else:\n",
    "        return 'neither'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company'] = df['tweet'].map(lambda x: brand_classifier(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apple      5275\n",
       "google     2781\n",
       "neither    786 \n",
       "both       250 \n",
       "Name: company, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['company'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - significantly better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in classifying negative vs. non-negative, let's reframe our data so that it only contains information about whether the tweet has negative sentiment or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_dict = {'Positive emotion':0,\n",
    "                 'No emotion toward brand or product':0,\n",
    "                 'I can\\'t tell':0,\n",
    "                 'Negative emotion':1}\n",
    "\n",
    "df['sentiment'] = df['emotion_response'].replace(emotions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   tweet             9092 non-null   object\n",
      " 1   product           3291 non-null   object\n",
      " 2   emotion_response  9092 non-null   object\n",
      " 3   company           9092 non-null   object\n",
      " 4   sentiment         9092 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 426.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8522\n",
       "1    570 \n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06269247690277167"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are most certainly dealing with a case of class imbalance, which means almost definitely that we will need to uppsample or downsample our data. Also, we should definitely be evaluating our models based on a metric different than accuracy. In the mean time, we'll drop the columns that are not necessary for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['product', 'emotion_response'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final preprocessing step, it is necessary to clean the tweet of all unnecessary characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    twtr_stopwords = ['rt','rts','retweet','quot','sxsw', 'amp']\n",
    "    punctuation = set(string.punctuation)\n",
    "    punctuation.remove('#')\n",
    "    \n",
    "    x = tweet\n",
    "    x = re.sub(r'https?:\\/\\/\\S+', '', x) #remove URLs\n",
    "    x = re.sub(r'{link}', '', x) #placeholders\n",
    "    x = re.sub(r'@[\\w]*', '', x) #@mention users\n",
    "    x = re.sub('[^A-Za-z0-9]+', ' ', x) #@mention users\n",
    "    x = re.sub(r'\\b[0-9]+\\b', '', x) #remove stand-alone numbers\n",
    "    x = re.sub(r'&[a-z]+;', '', x) #remove HTML ref chars\n",
    "    x = re.sub(r'\\d+', '', x) #removes all NUMERALS\n",
    "    x = ''.join(ch for ch in x if ch not in punctuation) #remove punctuation\n",
    "    x = x.replace(\"[^a-zA-z]#\", \" \") #remove special chars\n",
    "    \n",
    "    x = [word.lower() for word in x.split() if word.lower() not in twtr_stopwords]\n",
    "    x = [w for w in x if len(w)>2]\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    x = [lemmatizer.lemmatize(token) for token in x]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_clean'] = df['tweet'].map(lambda x: tweet_cleaner(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since we are interested in seeing if we can generalize our results, we will split the dataset according to brand. Additonally, we can drop all the columns except our tweet_clean column and our sentiment column, because those are the only relevant columns for building our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_apple = df[df['company']=='apple']\n",
    "df_google = df[df['company'] == 'google']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X = df['tweet']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rf_pipe = Pipeline(steps=[('preprocessing', CountVectorizer(lowercase=False, \n",
    "                                                            tokenizer=tweet_cleaner,\n",
    "                                                            max_features=100)),\n",
    "                          ('rf', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "rf_grid = {'rf__n_estimators': [20, 120, 220, 500],\n",
    "           'rf__max_depth': [3,6,9],\n",
    "           'rf__min_samples_split': [2, 5, 10],\n",
    "           'rf__min_samples_leaf': [1, 2, 4]\n",
    "          }\n",
    "\n",
    "rf_gs = GridSearchCV(estimator=rf_pipe,\n",
    "                     param_grid=rf_grid,\n",
    "                     cv=RepeatedStratifiedKFold(n_splits=3,\n",
    "                                                n_repeats=1,\n",
    "                                                random_state=42),\n",
    "                     verbose=100,\n",
    "                     n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   18.5s\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed:   27.6s\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   34.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   34.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed:   34.1s\n",
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed:   39.9s\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:   41.1s\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed:   42.2s\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed:   43.4s\n",
      "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed:   45.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 182 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   46.9s\n",
      "[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 186 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=-1)]: Done 187 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 190 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=-1)]: Done 191 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   48.7s\n",
      "[Parallel(n_jobs=-1)]: Done 193 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done 194 tasks      | elapsed:   49.6s\n",
      "[Parallel(n_jobs=-1)]: Done 195 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=-1)]: Done 199 tasks      | elapsed:   51.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:   51.6s\n",
      "[Parallel(n_jobs=-1)]: Done 201 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 202 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 203 tasks      | elapsed:   51.8s\n",
      "[Parallel(n_jobs=-1)]: Done 204 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 206 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 207 tasks      | elapsed:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:   53.3s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 211 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 212 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:   54.6s\n",
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 215 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:   54.8s\n",
      "[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed:   55.8s\n",
      "[Parallel(n_jobs=-1)]: Done 219 tasks      | elapsed:   56.0s\n",
      "[Parallel(n_jobs=-1)]: Done 220 tasks      | elapsed:   56.1s\n",
      "[Parallel(n_jobs=-1)]: Done 221 tasks      | elapsed:   56.2s\n",
      "[Parallel(n_jobs=-1)]: Done 222 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=-1)]: Done 223 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done 227 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=-1)]: Done 228 tasks      | elapsed:   58.0s\n",
      "[Parallel(n_jobs=-1)]: Done 229 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:   58.8s\n",
      "[Parallel(n_jobs=-1)]: Done 231 tasks      | elapsed:   59.0s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:   59.8s\n",
      "[Parallel(n_jobs=-1)]: Done 235 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 236 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 237 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 240 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 242 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 245 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 247 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 250 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 251 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 252 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 253 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 254 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 258 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 259 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 260 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 262 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:  1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 265 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 266 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 267 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 268 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 269 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 270 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 271 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 273 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 274 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 275 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 277 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 278 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 279 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 282 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 283 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 285 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 286 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 287 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 288 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 290 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 291 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 292 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 293 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 294 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 295 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 296 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 298 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 299 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 301 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 305 out of 324 | elapsed:  1.3min remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 309 out of 324 | elapsed:  1.3min remaining:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 313 out of 324 | elapsed:  1.3min remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 317 out of 324 | elapsed:  1.4min remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 321 out of 324 | elapsed:  1.4min remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedStratifiedKFold(n_repeats=1, n_splits=3, random_state=42),\n",
       "             estimator=Pipeline(steps=[('preprocessing',\n",
       "                                        CountVectorizer(lowercase=False,\n",
       "                                                        max_features=100,\n",
       "                                                        tokenizer=<function tweet_cleaner at 0x7fa5e93af3a0>)),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [3, 6, 9],\n",
       "                         'rf__min_samples_leaf': [1, 2, 4],\n",
       "                         'rf__min_samples_split': [2, 5, 10],\n",
       "                         'rf__n_estimators': [20, 120, 220, 500]},\n",
       "             verbose=100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9372341985628391"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375274967003959"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Research\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
